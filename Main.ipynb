{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import playsound\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "classifier = load_model('emotion_model_1.h5') # This model has a set of 6 classes\n",
    "\n",
    "class_labels = {0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n",
    "classes = list(class_labels.values())\n",
    "# print(class_labels)\n",
    "\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def text_on_detected_boxes(text,text_x,text_y,image,font_scale = 1,\n",
    "                           font = cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                           FONT_COLOR = (0, 0, 0),\n",
    "                           FONT_THICKNESS = 2,\n",
    "                           rectangle_bgr = (0, 255, 0)):\n",
    "\n",
    "\n",
    "\n",
    "    (text_width, text_height) = cv2.getTextSize(text, font, fontScale=font_scale, thickness=2)[0]\n",
    "    box_coords = ((text_x-10, text_y+4), (text_x + text_width+10, text_y - text_height-5))\n",
    "    cv2.rectangle(image, box_coords[0], box_coords[1], rectangle_bgr, cv2.FILLED)\n",
    "    cv2.putText(image, text, (text_x, text_y), font, fontScale=font_scale, color=FONT_COLOR,thickness=FONT_THICKNESS)\n",
    "\n",
    "def face_detector_image(img):\n",
    "    gray = cv2.cvtColor(img.copy(), cv2.COLOR_BGR2GRAY) # Convert the image into GrayScale image\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    print(faces)\n",
    "    if faces is ():\n",
    "        return (0, 0, 0, 0), np.zeros((48, 48), np.uint8), img\n",
    "\n",
    "    allfaces = []\n",
    "    rects = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "        allfaces.append(roi_gray)\n",
    "        rects.append((x, w, y, h))\n",
    "    return rects, allfaces, img\n",
    "\n",
    "\n",
    "def emotionImage(imgPath):\n",
    "    img = cv2.imread(imgPath)\n",
    "    rects, faces, image = face_detector_image(img)\n",
    "\n",
    "    i = 0\n",
    "    for face in faces:\n",
    "        roi = face.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "        preds = classifier.predict(roi)[0]\n",
    "        label = class_labels[preds.argmax()]\n",
    "        label_position = (rects[i][0] + int((rects[i][1] / 2)), abs(rects[i][2] - 10))\n",
    "        i = + 1\n",
    "\n",
    "\n",
    "        text_on_detected_boxes(label, label_position[0],label_position[1], image)\n",
    "\n",
    "\n",
    "    cv2.imshow(\"Emotion Detector\", image)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def face_detector_video(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0, 0, 0, 0), np.zeros((48, 48), np.uint8), img\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), thickness=2)\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        return (x, w, y, h), roi_gray, img\n",
    "\n",
    "\n",
    "def emotionVideo():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    frame_count=0\n",
    "    prev_em=\"\"\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        print(ret)\n",
    "        rect, face, image = face_detector_video(frame)\n",
    "        if np.sum([face]) != 0.0:\n",
    "            roi = face.astype(\"float\") / 255.0\n",
    "            roi = img_to_array(roi)\n",
    "            roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "            # make a prediction on the ROI, then lookup the class\n",
    "            preds = classifier.predict(roi)[0]\n",
    "            label = class_labels[preds.argmax()]\n",
    "            label_position = (rect[0] + rect[1]//50, rect[2] + rect[3]//50)\n",
    "            print(\"Emotion\"+str(label))\n",
    "            if label==prev_em:\n",
    "                frame_count+=1\n",
    "            else:\n",
    "                frame_count=0\n",
    "                prev_em=label\n",
    "            if frame_count>40:\n",
    "                if label=='Sad':\n",
    "                    playsound(\"sad.wav\")\n",
    "                elif label=='Happy':\n",
    "                    playsound(\"happy.wav\")\n",
    "                elif label=='Neutral':\n",
    "                    playsound(\"neutral.wav\")\n",
    "                elif label=='Angry':\n",
    "                    playsound(\"angry.wav\")\n",
    "                elif label=='Surprise':\n",
    "                    playsound(\"suprise.wav\")\n",
    "                \n",
    "\n",
    "            text_on_detected_boxes(label, label_position[0], label_position[1], image) # You can use this function for your another opencv projects.\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            cv2.putText(image, str(fps),(5, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        else:\n",
    "            cv2.putText(image, \"No Face Found\", (5, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "        cv2.imshow('All', image)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotionVideo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('All', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
